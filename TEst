company-overview-engine/
â”œâ”€ README.md
â”œâ”€ pyproject.toml                    # deps: pydantic, openai/azure, click, pyyaml, etc.
â”œâ”€ docs/
â”‚  â”œâ”€ architecture.md                # high-level design for company_overview engine
â”‚  â”œâ”€ prompt_lifecycle.md            # how prompts are versioned, tested, promoted
â”‚  â””â”€ eval_methodology.md            # how you evaluate company_overview quality
â”œâ”€ examples/
â”‚  â”œâ”€ sample_company_input.json      # mocked company data (bank / SaaS, etc.)
â”‚  â””â”€ sample_company_overview.json   # sample parsed output
â”œâ”€ src/
â”‚  â””â”€ company_overview_engine/
â”‚     â”œâ”€ __init__.py
â”‚     â”‚
â”‚     â”œâ”€ config/
â”‚     â”‚  â”œâ”€ company_overview.yaml    # core config for this section:
â”‚     â”‚  â”‚                          #   - prompt_name: company_overview
â”‚     â”‚  â”‚                          #   - active_version: v2025_02_15
â”‚     â”‚  â”‚                          #   - schema: company_overview_engine.schemas.company_overview.CompanyOverviewOutput
â”‚     â”‚  â”‚                          #   - use_case: company_overview
â”‚     â”‚  â”œâ”€ models.yaml             # model routing (which LLM to use when)
â”‚     â”‚  â”‚                          # e.g.:
â”‚     â”‚  â”‚                          # company_overview:
â”‚     â”‚  â”‚                          #   default_model: gpt-4.1-mini
â”‚     â”‚  â”‚                          #   long_doc_model: gpt-4.1
â”‚     â”‚  â”œâ”€ kpi_registry.py         # KPI dictionary (id, label, domain, tags, unit)
â”‚     â”‚  â”œâ”€ kpi_packs.yaml          # how to pick KPIs per industry/use_case
â”‚     â”‚  â”œâ”€ prompt_policies.yaml    # temp, max_tokens, retries, safety flags
â”‚     â”‚  â””â”€ logging.yaml            # logging/telmetry config
â”‚     â”‚
â”‚     â”œâ”€ prompts/
â”‚     â”‚  â””â”€ company_overview/
â”‚     â”‚     â”œâ”€ prompt_v2025_01_10.md    # initial prod prompt
â”‚     â”‚     â”œâ”€ prompt_v2025_02_15.md    # improved / few-shot tuned prompt
â”‚     â”‚     â””â”€ meta.yaml                # owner, notes, change log, default version
â”‚     â”‚
â”‚     â”œâ”€ schemas/
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ base.py                     # BaseModel config: forbid_extra, etc.
â”‚     â”‚  â””â”€ company_overview.py         # CompanyOverviewOutput Pydantic model:
â”‚     â”‚                                #   profile, strategic_position,
â”‚     â”‚                                #   financial_health, risks[], kpis[]
â”‚     â”‚
â”‚     â”œâ”€ engine/                        # runtime for company_overview only
â”‚     â”‚  â”œâ”€ llm_client.py               # OpenAI/Azure wrapper (keys, retries at HTTP level)
â”‚     â”‚  â”œâ”€ routing.py                  # chooses model from config/models.yaml
â”‚     â”‚  â”œâ”€ prompt_loader.py            # load & render company_overview prompt by version
â”‚     â”‚  â”œâ”€ guardrails.py               # JSON schema enforcement, safety / sanity checks
â”‚     â”‚  â”œâ”€ retry_policies.py           # retry on invalid JSON / truncation using guardrails
â”‚     â”‚  â””â”€ runtime.py                  # company_overview() entry:
â”‚     â”‚                                #   - load config
â”‚     â”‚                                #   - select KPIs
â”‚     â”‚                                #   - render prompt
â”‚     â”‚                                #   - call LLM via routing + llm_client
â”‚     â”‚                                #   - validate + return parsed + metadata
â”‚     â”‚
â”‚     â”œâ”€ eval/                          # offline evaluation for company_overview
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ datasets/
â”‚     â”‚  â”‚  â””â”€ company_overview.jsonl   # test cases: input + expectations
â”‚     â”‚  â”œâ”€ metrics.py                  # schema_ok, KPI accuracy, coverage, text checks
â”‚     â”‚  â”œâ”€ kpi_eval.py                 # numeric consistency vs ground truth (CET1, NIM, ARR, etc.)
â”‚     â”‚  â”œâ”€ text_eval.py                # simple expectations: key terms, risk coverage
â”‚     â”‚  â”œâ”€ llm_judge.py                # optional LLM-as-judge scoring (faithfulness, clarity)
â”‚     â”‚  â””â”€ run_tests.py                # CLI / VS startup:
â”‚     â”‚                                #   - iterate test dataset
â”‚     â”‚                                #   - call engine.runtime.company_overview()
â”‚     â”‚                                #   - compute metrics, write CSV (eval_results.csv)
â”‚     â”‚
â”‚     â”œâ”€ telemetry/
â”‚     â”‚  â”œâ”€ logging_config.py           # sets up structured logging (JSON logs if desired)
â”‚     â”‚  â”œâ”€ event_logger.py             # log per-call: version, model, latency, tokens, cost
â”‚     â”‚  â”œâ”€ cost_estimator.py           # estimate $ cost from token counts & model pricing
â”‚     â”‚  â””â”€ exporters/
â”‚     â”‚     â”œâ”€ console_exporter.py      # dev-mode printing
â”‚     â”‚     â”œâ”€ csv_exporter.py          # append prod/eval runs to CSV
â”‚     â”‚     â””â”€ prometheus_exporter.py   # optional metrics endpoint
â”‚     â”‚
â”‚     â”œâ”€ cli/
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ main.py                     # click/argparse:
â”‚     â”‚                                #   - `co-engine generate --industry banking --input examples/sample_company_input.json`
â”‚     â”‚                                #   - `co-engine eval` (wraps eval/run_tests.py)
â”‚     â”‚
â”‚     â””â”€ utils/
â”‚        â”œâ”€ json_utils.py               # safe JSON parsing / repair / minify
â”‚        â”œâ”€ time_utils.py               # latency measuring helpers
â”‚        â””â”€ typing_helpers.py           # shared type aliases / Result types
â”‚
â””â”€ tests/                               # pytest unit tests for this single section
   â”œâ”€ test_schema_company_overview.py   # Pydantic model behavior, required fields
   â”œâ”€ test_engine_prompt_loader.py      # correct version/file loaded, vars rendered
   â”œâ”€ test_engine_guardrails.py         # invalid JSON triggers retry / error
   â”œâ”€ test_eval_kpi_eval.py             # KPI numeric checks work
   â””â”€ test_cli_generate.py              # CLI generate command works end-to-end on mock

-----
You can now implement just the core path:
	1.	schemas/company_overview.py
	2.	config/company_overview.yaml
	3.	prompts/company_overview/prompt_v2025_02_15.md
	4.	engine/runtime.py
	5.	eval/run_tests.py
------
co-engine generate --industry banking --input examples/sample_company_input.json

-------
0. Entry point

File: src/company_overview_engine/cli/main.py
What it does:
	1.	Parses CLI args:
	â€¢	industry = "banking"
	â€¢	company_data = load JSON from examples/sample_company_input.json
	2.	Calls the core runtime:
from company_overview_engine.engine.runtime import company_overview

result = company_overview(industry=industry, company_data=company_data)

1. Runtime bootstraps config

File: src/company_overview_engine/engine/runtime.py

When company_overview() starts, it:
	1.	Loads section config:
	â€¢	File read: config/company_overview.yaml
	â€¢	From here it gets:
	â€¢	prompt_name: "company_overview"
	â€¢	active_version: "v2025_02_15"
	â€¢	schema: "company_overview_engine.schemas.company_overview.CompanyOverviewOutput"
	â€¢	use_case: "company_overview"
	2.	Loads model routing config:
	â€¢	File read: config/models.yaml
	â€¢	To decide which Azure model/deployment to hit for this request.

So at this point, runtime knows:
	â€¢	which prompt version to use,
	â€¢	which schema to validate against,
	â€¢	which Azure model/deployment to call.

â¸»

2. Select KPIs for the given industry

Still in:

File: engine/runtime.py
	1.	It imports KPI selection logic and configs:
	â€¢	Python module used: config/kpi_registry.py
	â€¢	File read: config/kpi_packs.yaml
	2.	It calls something like select_kpis_for_industry(use_case="company_overview", industry="banking") which:
	â€¢	filters KPIs from KPI_REGISTRY (e.g., banking.CET1, banking.NIM, etc.),
	â€¢	uses kpi_packs.yaml to cap/choose which ones to show in the overview.

kpis = [
  {"id": "banking.CET1", "name": "CET1 capital ratio", "unit": "%"},
  {"id": "banking.NIM", "name": "Net interest margin", "unit": "%"},
  ...
]
These are exactly the KPI parameters that will be injected into the prompt for that industry.

â¸»

3. Load and render the prompt

Files:
	â€¢	engine/prompt_loader.py
	â€¢	prompts/company_overview/prompt_v2025_02_15.md

Inside runtime.company_overview():
	1.	Call load_prompt(prompt_name="company_overview", version="v2025_02_15")
	â€¢	File read:
src/company_overview_engine/prompts/company_overview/prompt_v2025_02_15.md
This file contains your full template, including sections like:

# System
You are a senior financial analyst...

# Task
...

Industry: {{industry}}

Relevant KPIs:
{{kpi_descriptions}}

Company data:
{{company_data}}

Output format (MANDATORY)...



----------------
Cost to store get info from LLM
| LLM     | Embedding model | Dims | Embed cost | CAR LLM cost | **Total (first run)**                 |
| ------- | --------------- | ---- | ---------- | ------------ | ------------------------------------- |
| GPT-4.1 | 3-small         | 1024 | $0.009     | $0.56        | **$0.569**                            |
| GPT-4.1 | 3-large         | 1024 | $0.058     | $0.56        | **$0.618**                            |
| GPT-4.1 | 3-large         | 3072 | $0.058     | $0.56        | **$0.618** (+ extra RAM / index size) |
| GPT-5   | 3-small         | 1024 | $0.009     | $0.45        | **$0.459**                            |
| GPT-5   | 3-large         | 1024 | $0.058     | $0.45        | **$0.508**                            |
| GPT-5   | 3-large         | 3072 | $0.058     | $0.45        | **$0.508** (+ extra RAM / index size) |

Cost to convert 10k into embeddings
| Embedding model | Vector dims | #vectors | Vector size  | One-time embed cost | Monthly storage (â‰ˆ$0.25/GB) |
| --------------- | ----------- | -------- | ------------ | ------------------- | --------------------------- |
| 3-small         | **1024**    | 556      | **2.17 MiB** | **$0.009**          | **$0.0005**                 |
| 3-small         | **1536**    | 556      | **3.26 MiB** | **$0.009**          | **$0.0008**                 |
| 3-small         | **3072**    | â€“        | â€“            | **N/A (max 1536)**  | â€“                           |
| 3-large         | **1024**    | 556      | **2.17 MiB** | **$0.058**          | **$0.0005**                 |
| 3-large         | **3072**    | 556      | **6.52 MiB** | **$0.058**          | **$0.0016**                 |


| Metric                    | **GPT-4.1**                                   | **GPT-5.1**                                                                                                    |
| ------------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Role in pipeline**      | Fast RAG Q&A & summaries                      | Deep CAR generation & multi-section analysis                                                                    |
| **Context window**        | **~1,047,576 tokens** ([OpenAI Platform][1])  | **400,000 tokens** ([OpenAI Platform][2])                                                                      |
| **Quality / reasoning**   | Strong summarizer; good single-step reasoning | Higher-end reasoning; better on complex, multi-hop tasks (beats 4.1 on AIME, GPQA, MMMU etc.) ([LLM Stats][3]) |
| **Input price (per 1M)**  | **$2.00**                                     | **$1.25** ([OpenAI Platform][1])                                                                               |
| **Cached input (per 1M)** | **$0.50**                                     | **$0.13** ([OpenAI Platform][1])                                                                               |
| **Output price (per 1M)** | **$8.00**                                     | **$10.00** ([OpenAI Platform][1])                                                                              |
| **Latency profile**       | Lower TTFT; better for high-QPS chat          | Higher TTFT (adaptive reasoning) but fewer reruns for complex asks                                             |

[1]: https://platform.openai.com/docs/models/compare?model=gpt-4.1&utm_source=chatgpt.com "Compare models - OpenAI API"
[2]: https://platform.openai.com/docs/models/compare?model=gpt-5.1&utm_source=chatgpt.com "Compare models - OpenAI API"
[3]: https://llm-stats.com/models/compare/gpt-4.1-2025-04-14-vs-gpt-5.1-thinking-2025-11-12?utm_source=chatgpt.com "GPT-4.1 vs GPT-5.1 Thinking"

4.1 = bigger desk, average analyst
5.1 = slightly smaller desk, much smarter analyst, cheaper hourly rate
5.1 is trained as a reasoning model: better at
multi-step logic,reconciling conflicting numbers,tracing dependencies across different sections (e.g., footnotes â†” MD&A â†” cash flow).
â€œTell me how liquidity, capital ratios, and segment revenue tie together, and point to the evidence.â€ We rarely hit the context limit in CAR RAG, but you constantly hit reasoning complexity.

0. Assumptions (so you can sanity-check)

250,000 words Ã— 1.33 tokens/word â‰ˆ 332,500 tokens
-----------------------
What does â€œ556 vectorsâ€ actually mean?

In the example we were using:
Chunk size: 800 tokens
Overlap: 200 tokens (stride = 600)
#chunks (â‰ˆ #vectors): 556
So:
556 vectors Ã— 800 tokens/vector = 444,800 tokens
Now convert tokens â†’ words using our rule of thumb:

1 token â‰ˆ 0.75 words
â‡’ words â‰ˆ tokens Ã— 0.75

So:

444,800 tokens Ã— 0.75 â‰ˆ 333,600 words
Answer:
ğŸ‘‰ 556 vectors â‰ˆ 444,800 tokens â‰ˆ ~334k words of text as seen by the embedding API.

Why is that more than the 250k words we estimated for the 10-K?

Because with overlap:

Each chunk repeats some text (the 200-token overlap with the previous chunk),

So the embeddings API â€œseesâ€ more tokens than the number of unique tokens/words in the document.

Roughly:

Unique doc content: ~250k words (~333k tokens)

Embedding workload (with overlap): ~334k wordsâ€™ worth of tokens (444.8k tokens)

So 556 vectors is not â€œ556 separate pagesâ€; itâ€™s 556 overlapping slices of the same 10-K.
--------------------------------------------------------------------------------------------
Document size

500 pages Ã— 500 words/page â‰ˆ 250,000 words.

Rule of thumb: 0.75 words/token â†’ tokens â‰ˆ 250,000 / 0.75 â‰ˆ 333,000 tokens.

Chunking for RAG

Chunk size = 800 tokens, overlap = 200 â†’ stride = 600.

#chunks â‰ˆ 556

Total tokens fed to embeddings = 556 Ã— 800 = 444,800 tokens (â‰ˆ0.445M).

Embedding prices (OpenAI list)

text-embedding-3-small: $0.02 / 1M tokens

text-embedding-3-large: $0.13 / 1M tokens

Default dims: 3-small = 1536; 3-large = 3072. Both can be shortened using dimensions (e.g. 1024).

Important: 3-small cannot go to 3072 dims; max is 1536. 3072 only applies to 3-large.

Vector storage math

Assume float32: 4 bytes / dim.

Size = #chunks Ã— dims Ã— 4 bytes.

LLM usage for CAR generation

To keep this simple and defensible, assume your full CAR run over that 10-K uses in total:

200k input tokens (sum of all prompts across sections)

20k output tokens (all final CAR text)

You can scale the numbers linearly if you end up with 100k / 10k or 400k / 40k instead.

LLM prices (OpenAI list)

GPT-4.1:

Input: $2.00 / 1M tokens, Output: $8.00 / 1M tokens

GPT-5 (what you called 5.0):

Input: $1.25 / 1M tokens, Output: $10.00 / 1M tokens

1. Embedding + Vector Storage per JPM 10-K
1A. One-time embedding cost (per 10-K)

Using 444,800 embedding tokens (~0.445M):

3-small: 0.445M Ã— $0.02 â‰ˆ $0.0089

3-large: 0.445M Ã— $0.13 â‰ˆ $0.0578

So:

3-small: â‰ˆ $0.009 to index one JPM 10-K

3-large: â‰ˆ $0.058 to index one JPM 10-K

This does not depend on 1024 vs 3072 dims â€“ only on tokens.

1B. Vector size & storage â€“ 1024 vs 3072 dims

For 556 vectors:

1024 dims:

Size = 556 Ã— 1024 Ã— 4 bytes â‰ˆ 2.17 MiB

1536 dims (default 3-small):

Size â‰ˆ 3.26 MiB

3072 dims (default 3-large):

Size â‰ˆ 6.52 MiB

If you price storage purely as $0.25 / GB-month (rough Azure storage ballpark):

2.17 MiB â‰ˆ 0.0021 GB â†’ ~$0.0005 / month

3.26 MiB â‰ˆ 0.0032 GB â†’ ~$0.0008 / month

6.52 MiB â‰ˆ 0.0064 GB â†’ ~$0.0016 / month

So per 10-K, vector storage is fractions of a cent per month â€“ the real cost is the search unit (Basic/S1), not the incremental file.

1C. Summary table: embedding + storage (per 10-K)
Embedding model	Vector dims	#vectors	Vector size	One-time embed cost	Monthly storage (â‰ˆ$0.25/GB)
3-small	1024	556	2.17 MiB	$0.009	$0.0005
3-small	1536	556	3.26 MiB	$0.009	$0.0008
3-small	3072	â€“	â€“	N/A (max 1536)	â€“
3-large	1024	556	2.17 MiB	$0.058	$0.0005
3-large	3072	556	6.52 MiB	$0.058	$0.0016

Big picture: going from 1024 â†’ 3072 dims on 3-large triples vector storage per 10-K, but your per-document $ impact is still tiny. The real trade-off is index capacity & RAM at scale (thousands of filings).

2. LLM Cost to Generate CAR per JPM 10-K

Using the earlier assumption:

200,000 input tokens total

20,000 output tokens total

2A. GPT-4.1

Input: 0.2M Ã— $2.00 = $0.40

Output: 0.02M Ã— $8.00 = $0.16

Total per CAR (4.1): â‰ˆ $0.56

2B. GPT-5

Input: 0.2M Ã— $1.25 = $0.25

Output: 0.02M Ã— $10.00 = $0.20

Total per CAR (5): â‰ˆ $0.45

So for one 500-page JPM 10-K, at this usage level:

GPT-4.1 CAR run: ~$0.56

GPT-5 CAR run: ~$0.45

And you can scale linearly:

If you only use 100k / 10k tokens â†’ divide by 2

If you use 400k / 40k tokens â†’ multiply by 2

3. Combined per-filing view (embed + CAR generation)

Per JPM 10-K, one-time embedding + one full CAR generation:

LLM	Embedding model	Dims	Embed cost	CAR LLM cost	Total (first run)
GPT-4.1	3-small	1024	$0.009	$0.56	$0.569
GPT-4.1	3-large	1024	$0.058	$0.56	$0.618
GPT-4.1	3-large	3072	$0.058	$0.56	$0.618 (+ extra RAM / index size)
GPT-5	3-small	1024	$0.009	$0.45	$0.459
GPT-5	3-large	1024	$0.058	$0.45	$0.508
GPT-5	3-large	3072	$0.058	$0.45	$0.508 (+ extra RAM / index size)

Key takeaways you can say out loud to a tech-heavy business:

Indexing cost per 10-K is pennies (1Â¢ with 3-small, 6Â¢ with 3-large).

Vector storage per 10-K is a few MB â†’ << 1 cent/month, even at 3072 dims.

The real money is the LLM:

Around $0.45â€“0.60 per full CAR at 200k/20k tokens.

GPT-5 is ~20% cheaper than GPT-4.1 at that usage level and gives you better reasoning.

If you want, next step I can redo this with your actual CAR token counts (once you have logs) so you can show a â€œthis is what weâ€™re really paying per filingâ€ slide to UPH.
