
	2.	Call render_prompt(...) in prompt_loader.py with:
	•	industry
	•	kpis
	•	company_data
prompt_loader.render_prompt():
	•	builds kpi_descriptions from selected KPIs (e.g., bullet list),
	•	JSON-dumps company_data,
	•	replaces placeholders:
	•	{{industry}}
	•	{{kpi_descriptions}}
	•	{{company_data}}
→ Returns a final prompt string ready to send to Azure.

At this point, runtime has a fully filled-in prompt string with:
	•	right industry,
	•	right KPI list,
	•	actual company data.

⸻

4. Route to the right Azure model

Files:
	•	engine/routing.py
	•	config/models.yaml

runtime.company_overview() now calls:

from company_overview_engine.engine.routing import choose_model

model_info = choose_model(
    section="company_overview",
    company_data=company_data,
)

------
routing.py:
	1.	Reads config/models.yaml (if not already cached) to get rules like:

company_overview:
  default_model: "azure-gpt-4.1-mini"
  long_doc_model: "azure-gpt-4.1"
  long_doc_threshold_tokens: 4000

2.	Maybe inspects company_data size or complexity.
	
3.	Returns something like:
model_info = {
  "azure_deployment": "gpt4-mini-ovw",
  "max_tokens": 1200,
  "temperature": 0.1
}
So now runtime knows which Azure deployment and generation parameters to use.

5. Call Azure (the actual LLM call)

File: engine/llm_client.py

Inside runtime:
from company_overview_engine.engine.llm_client import call_llm

raw_response = call_llm(
    prompt_text=rendered_prompt,
    model_info=model_info,
)
llm_client.py does the Azure-specific stuff:
	1.	Reads Azure config from env or settings:
	•	AZURE_OPENAI_ENDPOINT
	•	AZURE_OPENAI_KEY
	•	deployment_name from model_info["azure_deployment"]
	2.	Builds Azure-style request:
	•	Wraps content into messages, e.g.:

messages = [
  {"role": "user", "content": rendered_prompt}
]

	•	Calls Azure Chat Completions:
client = AzureOpenAI(
    api_key=...,
    azure_endpoint=...,
)

resp = client.chat.completions.create(
    model=model_info["azure_deployment"],
    messages=messages,
    max_tokens=model_info["max_tokens"],
    temperature=model_info["temperature"],
)
3.	Returns the raw text:
raw_text = resp.choices[0].message.content
This raw_text should be the JSON string the prompt asked for.
6. Guardrails: validate & retry if needed

Files:
	•	engine/guardrails.py
	•	engine/retry_policies.py
	•	schemas/company_overview.py
	•	schemas/base.py

Back in runtime.company_overview():
	1.	It imports schema:
	•	CompanyOverviewOutput class from schemas/company_overview.py
	•	That inherits from base config in schemas/base.py (forbid extra fields etc.)
	2.	It calls guardrails:

from company_overview_engine.engine.guardrails import validate_and_fix

parsed_obj, final_raw = validate_and_fix(
    raw_text,
    schema_model=CompanyOverviewOutput,
)
3.	guardrails.validate_and_fix():
	•	Tries to json.loads(raw_text).
	•	Validates it against CompanyOverviewOutput (Pydantic):
	•	ensure fields exist: profile, strategic_position, financial_health, risks, kpis…
	•	ensure kpis is a list of {id, name, value, unit, interpretation}.
	•	If invalid:
	•	It uses retry_policies.py to either:
	•	Send a “fix your JSON” follow-up request to the same model,
	•	Or re-call the original prompt with stricter instructions.
	•	After N retries, it fails.
	4.	If success:
	•	parsed_obj is a fully typed CompanyOverviewOutput instance.
	•	final_raw is the final JSON string that passed validation.

⸻

7. Telemetry & return

Files:
	•	telemetry/event_logger.py
	•	telemetry/cost_estimator.py

Still in runtime.company_overview():
1.	It logs the call:
from company_overview_engine.telemetry.event_logger import log_event
from company_overview_engine.telemetry.cost_estimator import estimate_cost

cost = estimate_cost(raw_text, model_info)
log_event(
  section="company_overview",
  prompt_version="v2025_02_15",
  model=model_info["azure_deployment"],
  latency_ms=...,           # measured via time_utils
  tokens_prompt=...,        # from Azure response usage
  tokens_completion=...,    # from Azure response usage
  cost_estimate=cost,
  success=True,
)
	2.	Finally, company_overview() returns something like:
return {
  "parsed": parsed_obj,   # Pydantic object
  "raw": final_raw,       # JSON string
  "metadata": {
     "industry": industry,
     "prompt_version": "v2025_02_15",
     "model": model_info["azure_deployment"],
  },
}

The CLI main.py prints this / writes to file.

⸻

Summary in 1 line

Order of files:

cli/main.py → engine/runtime.py → (config/*.yaml + config/kpi_registry.py + config/kpi_packs.yaml) → engine/prompt_loader.py → engine/routing.py → engine/llm_client.py (Azure call) → engine/guardrails.py + schemas/company_overview.py + engine/retry_policies.py → telemetry/* → back to cli/main.py.

That’s the full path:
prompt template → industry-specific KPI selection → rendered context → Azure call → validated JSON output.


