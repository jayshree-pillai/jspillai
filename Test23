So the runtime chain is now:

1:CLI entry

src/prompt_lifecycle/cli/main.py

co-engine generate ... calls

from prompt_lifecycle.engine.runtime import company_overview


2:Core runtime

src/prompt_lifecycle/engine/runtime.py

Inside here you:

load config:  
       config/company_overview.yaml
       config/models.yaml
      config/kpi_registry.py
      config/kpi_packs.yaml

select KPIs
    load prompt & render:
    engine/prompt_loader.py
          prompts/company_overview/prompt_v2025_02_15.md

3:Model routing + LLM call

pick model: engine/routing.py (reads config/models.yaml)

call Azure/OpenAI: engine/llm_client.py

4:Guardrails & schema validation

    JSON repair/validation: engine/guardrails.py
    retry policies (if you add them): engine/retry_policies.py
    schema: schemas/company_overview.py

5:Telemetry

telemetry/event_logger.py

telemetry/cost_estimator.py

(plus telemetry/logging_config.py to set up logging)

6: Return to CLI

company_overview() returns {parsed, raw, metadata}

cli/main.py prints or saves parsed.

So conceptually your pipeline is still:

cli → runtime → config/KPIs → prompt_loader → routing → llm_client → guardrails+schema → telemetry → cli
