Feature: Offline evaluation of a section prompt version against a dataset
  Background:
    Given offline evaluation is available for section outputs
    And datasets are stored and discoverable by dataset_id (or file id/path)

  Scenario: Run eval with explicit section, prompt version, and dataset
    Given section_id "company_overview" exists
    And prompt_version "v2026_01_05" exists for section_id "company_overview"
    And dataset_id "ds_credit_core_v1" exists
    When I run an offline eval with section_id "company_overview", prompt_version "v2026_01_05", dataset_id "ds_credit_core_v1"
    Then the eval uses the same prompt-loading mechanism as runtime
    And the eval produces a machine-readable results artifact containing:
      | section_id     |
      | prompt_version |
      | dataset_id     |
      | metrics        |
      | pass_fail      |
      | timestamp      |

  Scenario: Run eval with default dataset when dataset_id is omitted
    Given section_id "company_overview" exists
    And prompt_version "v2026_01_05" exists for section_id "company_overview"
    And a default dataset is configured for section_id "company_overview"
    When I run an offline eval with section_id "company_overview" and prompt_version "v2026_01_05" without dataset_id
    Then the eval uses the configured default dataset
    And the eval produces a machine-readable results artifact

  Scenario: Pass/fail is derived deterministically from configured thresholds
    Given an eval threshold configuration exists for section_id "company_overview"
    And the thresholds are not hardcoded in the evaluator
    When an offline eval completes
    Then pass_fail is computed deterministically from the produced metrics and the configured thresholds
    And the results artifact includes the computed pass_fail

  Scenario: Fail fast when dataset is missing or invalid
    Given section_id "company_overview" exists
    And prompt_version "v2026_01_05" exists for section_id "company_overview"
    When I run an offline eval with dataset_id "ds_missing"
    Then the eval fails with a clear error
    And the error includes the attempted dataset reference and available dataset ids (or how to list them)
    And no results artifact is produced
