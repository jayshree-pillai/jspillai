As an Eval Engineer, I want offline evaluations to produce both quantitative scores and qualitative assessments, so I can measure performance and understand why a prompt version is better before promotion.

Acceptance criteria (noise-cut, implementation-agnostic)

Eval produces quantitative metrics (numeric) per example and aggregated (e.g., schema pass rate, coverage %, citation/grounding rate, hallucination flags count, length/format compliance, etc.).

Eval produces qualitative scoring per example and aggregated (e.g., rubric-based ratings like 1â€“5 for clarity, correctness, completeness, conciseness, tone).

A single machine-readable results artifact includes:

identifiers: section_id, prompt_version, dataset_id

quant_metrics (per-example + aggregate)

qual_scores (per-example + aggregate)

overall pass_fail driven by configured thresholds on the quantitative metrics (and optionally minimum qualitative rating thresholds if you choose).

Scoring method/rubric version used for qualitative scoring is recorded in the results artifact.

If scoring cannot be computed (missing rubric/config/dataset), eval fails with a clear error.
